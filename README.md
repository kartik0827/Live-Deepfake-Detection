# ğŸ¯ Live Deepfake Detection â€” Fusion Scanner

A real-time deepfake detection system that **automatically hunts for faces on screen**, locks onto them with a sniper-scope overlay, and runs **dual visual + audio analysis pipelines** simultaneously.

![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c)
![License: MIT](https://img.shields.io/badge/License-MIT-green)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Auto-Tracking** | MTCNN-based full-screen face detection with smooth lerp tracking |
| **Visual Pipeline** | EfficientNetV2-S feature extraction â†’ GRU temporal classifier |
| **Audio Pipeline** | Live microphone capture â†’ AASIST anti-spoofing model |
| **Score Fusion** | Weighted combination (70% visual + 30% audio) for robust detection |
| **Sniper-Scope UI** | Transparent PyQt6 overlay with crosshairs, spinning arcs, and colour-coded feedback |
| **Manual Mode** | Press `M` to switch to manual drag-to-position mode |

### Colour States
- âšª **Grey** â€” Searching for a face
- ğŸŸ¡ **Yellow** â€” Face acquired, filling frame buffer
- ğŸŸ¢ **Green** â€” Fusion says **REAL**
- ğŸ”´ **Red** â€” Fusion says **FAKE**

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FusionOverlay (UI)                 â”‚
â”‚         Transparent sniper-scope window              â”‚
â”‚         Fuses visual + audio scores                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ GlobalScan  â”‚  â”‚ ModelThread â”‚  â”‚ AudioScan  â”‚
    â”‚ Thread      â”‚  â”‚            â”‚  â”‚ Thread     â”‚
    â”‚             â”‚  â”‚ EfficientNetâ”‚  â”‚            â”‚
    â”‚ Full-screen â”‚  â”‚ + GRU on   â”‚  â”‚ PyAudio +  â”‚
    â”‚ MTCNN face  â”‚  â”‚ locked     â”‚  â”‚ AASIST on  â”‚
    â”‚ hunting     â”‚  â”‚ region     â”‚  â”‚ microphone â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Prerequisites

- **Python** 3.10 or higher
- **CUDA** (optional, recommended for GPU acceleration)
- **Microphone** for audio pipeline
- A screen with a face visible (video call, photo, etc.)

---

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/bps-rajora/Live-Deepfake-Detection.git
cd Live-Deepfake-Detection
```

### 2. Create a virtual environment

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / macOS
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
pip install timm facenet-pytorch pyaudio
```

### 4. Download model weights

The pre-trained weights are **not included** in this repo due to file-size limits. Download them and place into `models/weights/`:

| Model | File | Destination |
|-------|------|-------------|
| EfficientNetV2-S (video) | `best_ffpp_efficientnet.pth` | `models/weights/VIDEO/` |
| GRU temporal head | `best_rnn.pt` | `models/weights/VIDEO/` |
| AASIST (audio) | `AASIST.pth` | `models/weights/AASIST/` |
| AASIST-L (audio, lighter) | `AASIST-L.pth` | `models/weights/AASIST/` |

> **Note:** The application will still launch if weights are missing â€” the corresponding pipeline will simply show "OFF".

### 5. Download MediaPipe model files

Place these in the project root:
- `face_landmarker.task` â€” [Download from Google](https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task)
- `blaze_face_short_range.tflite` â€” [Download from Google](https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/latest/blaze_face_short_range.tflite)

### 6. Run

```bash
# Full Fusion Scanner (visual + audio)
python FusionScanner.py

# Visual-only Auto Tracker
python AutoTracker.py
```

---

## âŒ¨ï¸ Controls

| Key | Action |
|-----|--------|
| `M` | Toggle Manual / Auto tracking mode |
| `+` / `-` | Resize the scanner overlay |
| `R` | Reset the frame buffer |
| `Esc` | Quit |
| **Drag** | Move the overlay (Manual mode) |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ FusionScanner.py        # Main app: visual + audio fusion scanner
â”œâ”€â”€ AutoTracker.py          # Visual-only auto-tracking scanner
â”œâ”€â”€ AASISTMODEL.py          # AASIST model wrapper for live audio inference
â”œâ”€â”€ realtime_inference.py   # Standalone real-time inference script
â”œâ”€â”€ main.py                 # AASIST training / evaluation entry point
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ AASIST.py           # AASIST model architecture
â”‚   â”œâ”€â”€ RawNet2Spoof.py     # RawNet2 baseline model
â”‚   â”œâ”€â”€ RawNetGatSpoofST.py # RawGAT-ST baseline model
â”‚   â””â”€â”€ weights/            # Pre-trained weights (not tracked by git)
â”‚       â”œâ”€â”€ AASIST/
â”‚       â””â”€â”€ VIDEO/
â”œâ”€â”€ config/                 # Training config files (.conf)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ NOTICE                  # Third-party attributions
```

---

## ğŸ™ Acknowledgements

This project builds upon:

- **[AASIST](https://github.com/clovaai/aasist)** â€” Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks (NAVER Corp.)
- **[ASVspoof 2019](https://www.asvspoof.org/)** â€” Large-scale public database of synthesized, converted, and replayed speech
- **[EfficientNetV2](https://github.com/huggingface/pytorch-image-models)** â€” via `timm` library
- **[MTCNN](https://github.com/timesler/facenet-pytorch)** â€” via `facenet-pytorch`

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

Original AASIST code: Copyright (c) 2021-present NAVER Corp. â€” see [NOTICE](NOTICE).
